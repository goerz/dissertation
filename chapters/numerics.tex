% vim: ft=tex iskeyword=@,48-57,_,-,192-255,\: dictionary=bibkeys.lst,labels.lst:
\chapter{Numerical Methods in Quantum Information Processing}
\label{chap:numerics}

The  design of robust quantum gates under realistic conditions provides
considerable numerical challenges.
Even with a focus on two-qubit gates, it is generally not sufficient to model
the system simply in the four-dimensional logical subspace spanned by the states
\{\ket{00}, \ket{01}, \ket{10}, \ket{11}\}. For an accurate description of the
underlying physical system, more degrees of freedom need to be taken
into account. For example, for the case of trapped Rydberg atoms in
chapter~\ref{chap:robust}, the implementation of the gate makes use of a Rydberg
level \ket{r} and one more intermediate level \ket{i}, outside of the logical
subspace, in order to generate the entanglement necessary for a two-qubit gate.
Therefore, the minimum dimension of the total Hilbert space grows to 16. More
generally, for trapped atoms, it may be necessary to also include
their vibrational degree of freedom, which may further enlarge the Hilbert space
by a factor of several hundred~\cite{GoerzJPB11, GoerzDipl10}. For
superconducting qubits discussed in
chapters~\ref{chap:transmon}--\ref{chap:3states}, the qubits are weakly
anharmonic ladders, such that the population of higher levels cannot be
neglected. Furthermore, the interaction between the qubit is via a transmission
line resonator, which can receive significant excitation. With $n_q$ levels per
qubit and $n_c$ levels for the resonator having to be taken into account, the
total dimension of the Hilbert space $n_q^2 n_c$ is generally at least several
hundred.

The problem is exacerbated when taking into account dissipation, such that the
system has to be modeled as a density matrix in Liouville space, which scales
quadratically relative to the underlying Hilbert space. Thus while Hilbert space
dimensions of several hundred are numerically very manageable, dimensions
approaching $10^5$ that would result in Liouville space push the boundaries of
feasibility, given current computational resources.

Therefore, it is vital to employ efficient data structures and algorithms that
are problem-specific to the simulation of quantum dynamics and control, using
highly optimized implementations.
In the context of the work presented in this thesis, considerable effort has
been spent to implement algorithms for the efficient representation, simulation,
and optimization of both closed and open quantum systems, as part of the QDYN
Fortran 90 library. This chapters reviews some of the key techniques that have
been implemented and used in obtaining the results in
chapters~\ref{chap:robust}--\ref{chap:3states}.


\section{Numerical Representation of Quantum Systems}

At the heart of any numerical treatment is the ability to
model the physical system in an optimal mathematical representation, and to
translate this mathematical model into efficient data structures.
It is important to realize how intimately efficient algorithms depend on
the underlying data structures, which must therefore be designed with great care.

The first step in modeling a physical system numerically is to discretize it to
finite degrees of freedom. In practice, this means choosing a suitable
representation for the Hamiltonian (i.e.\ a set of basis functions).
Naturally, the number of degrees of freedom should be
as small as possible, while still providing an accurate description.

If only bound states of the Hamiltonian up to some maximum energy are relevant,
an energy representation $\Braket{\phi_i | \Op{H} | \phi_j}$ is an obvious
choice, where $\{\ket{\phi_1} \dots \ket{\phi_N}\}$ is the set of the first $N$
eigenstates of the drift Hamiltonian. The resulting matrix representation is
almost always very sparse. Consider a typical Hamiltonian of the form
\begin{equation}
  \Op{H}(t) = \Op{H}_0 + \epsilon(t) \Op{H}_1\,.
\end{equation}
The drift Hamiltonian $\Op{H}_0$ is entirely diagonal, since $\Op{H}(t)$ is
represented in the eigenstates of $\Op{H}_0$, but also the control Hamiltonian
$\Op{H}_1$ will generally have a sparse structure. For example if $\Op{H}_1$ is
the dipole operator that describes the interaction of an atom with an
electromagnetic field, see appendix~\ref{AppendixLMI}, the non-zero elements in
$\Op{H}_1$ are determined by the selection rules.
Sparsity is further increased in a composite system
\begin{equation}
  \Op{H} = \Op{H}_1 \otimes \unity + \unity \otimes \Op{H}_1 + \Op{H}_I\,,
  \label{eq:num_H_composite}
\end{equation}
where $\Op{H}_1$ is the Hamiltonian for each of the two subsystems, assuming
that the interaction Hamiltonian $\Op{H}_I$ is also sparse.
Data structures for storing sparse matrices are
well-established~\cite{Usman2006, DuffSparseBook1989}, and are at least partly
available in standard numerical libraries such as LAPACK~\cite{LapackUG}. Where
these existing data structures are too general, it is advisable to implement
data structures that reflect the sparsity of the representation to the greatest
extent possible.  The possibility to efficiently apply a Hamiltonian
to a wave function is key to any further numerical methods of time propagation
or optimal control.

The same sparsity arguments applies equally in Liouville space. Given a master
equation~\eqref{eq:MELindblad} in Lindblad form, the commutator
$[\Op{H}, \Op{\rho}]$ can be efficiently calculated if $\Op{H}$ is stored in
a sparse matrix format. Likewise, the Lindblad operators $\{\Op{A}_i\}$ or
alternatively the entire dissipator $\Liouville_D$ can be stored as a sparse
matrices.

While the energy representation is very straightforward, it is neither the only
nor always the most efficient choice. In the standard example of
the harmonic oscillator~\cite{SakuraiBook},
\index{harmonic oscillator}%
\begin{equation}
  \Op{H}_{ho} = \hbar \omega \left(\Op{a}^\dagger \Op{a} + \frac{1}{2}\right)\,,
\end{equation}
$\Op{a}$ can be written in either energy representation (truncated
to $N$ eigenstates) or coordinate representation,
\begin{equation}
  \Op{a} = \sum_{n=0}^{N} \sqrt{n} \KetBra{n-1}{n}
         = \sqrt{\frac{m \omega_c}{2}}
           \left(\Op{x} + \frac{\ii \Op{p}}{m \omega_c}\right)\,,
\end{equation}
leading to
\begin{equation}
  \Op{H}_{ho}
  = \sum_{n=0}^{N} \hbar \omega \left(n + \frac{1}{2}\right) \KetBra{n}{n}
  = \frac{\Op{p}^2}{2m} + \frac{m \omega^2}{2} \Op{x}^2\,.
\end{equation}
While the energy representation is already discrete and finite, by choice
of the truncated set of energy basis functions $\{ \ket{0} \dots \ket{N} \}$,
the coordinate representation must still be discretized for numerical use. That
is, we must also select a finite set of basis functions in which to represent
the Hamiltonian. The Fourier method~\cite{KosloffJCP88, TannorBook} defines
\index{Fourier method}%
a \emph{pair} of such sets. One set consists of the ``spectral'' basis of plane
waves, $\ket{n} = \ket{\ee^{\ii k_n x}}$, the other of a basis of associated
``pseudo-spectral'' functions~\cite{TannorBook, GoerzDipl10}. In practical terms,
the representation of a state in the pseudo-spectral basis is simply the vector
$(\Psi(x_i), \dots , \Psi(x_N))$ for $x_i$ on an equidistant grid. The operator
$\Op{x}^2$ in this basis is the diagonal matrix $\Op{x}^2 = \sum_n x_n^2
\KetBra{n}{n}$, whereas $\Op{p}^2$ is a dense matrix.  The representation in the
plane wave basis is the Fourier transform of $\Psi(x_i)$, and consequently
$\Op{p}^2$ is diagonal, whereas $\Op{x}^2$ is dense. This suggests to split the
Hamiltonians into two terms, the kinetic operator $\Op{T} = \Op{p}^2/(2m)$ and
the harmonic potential $\Op{V} = \frac{1}{2} m \omega^2 \Op{x}^2$ and to
store each in the representation in which it is sparse (i.e.\ diagonal). In the
application of the total Hamiltonian, the state vector or density matrix must
then be converted from one representation to the other, which can be done
efficiently using the fast-Fourier-transform (FFT),
cf.~appendix~\ref{AppendixFFT}.
\index{Fast-Fourier-transform}%
The necessary number of grid points in $x$ and $p$ is completely determined by
the phase space volume $[x_{\min}, x_{\max}] \times [p_{\min}, p_{\max}]$ in
which the dynamics take place, see appendix~\ref{AppendixFFT}. The Fourier
representation fills this volume with the minimal number of equally distributed
points.

In the example of the harmonic oscillator, the Fourier representation only
becomes useful if the system is driven with a term $\epsilon(t)(\Op{a}
+ \Op{a}^\dagger)$. In this case, the energy representation
becomes banded instead of diagonal, whereas the Fourier representation remains
diagonal. Which representation is more efficient then depends on the maximum
excitation; for $n>32$, the Fourier grid is generally more efficient.
However, in the description of molecular dynamics, where the method was
developed~\cite{KosloffJCP88, Kosloff96}, the Fourier representation is the
default choice. There, the Born-Oppenheimer approximation yields energy surfaces
$\Op{V}_{BO}(R)$ for all of the electronic degrees of freedom of a diatomic
molecule. For two electronic surfaces, the Hamiltonian might be written as e.g.\
\begin{equation}
  \Op{H} = \begin{pmatrix}
    \Op{T}(k) + \Op{V}_1(R)      & \mu(R) \epsilon(t)      \\
    \Op{\mu}(R) \epsilon(t)      & \Op{T}(k) + \Op{V}_2(R)
  \end{pmatrix}
\end{equation}
With the spatial degree of freedom represented on a Fourier grid with $N_R$
points, \Op{H} is a sparse matrix of dimension $2 N_R$, and can be
applied efficiently by storing only the vectors $V_1(R)$, $V_2(R)$, and $\mu(R)$
in the pseudo-spectral representation and the vector $T(k) \propto k^2$ for the
kinetic operator in the plane wave representation.  For a more realistic
example, see Ref.~\cite{TomzaPRA2012}.

A common situation for such molecular systems is that the phase space is used
very unevenly, requiring an excessive number of points to sample it evenly in
the Fourier representation. This issue can be addressed by using a mapped
grid~\cite{FattalPRE96,KallushCPL06,WillnerJCP04,KokooulineJCP99,NestCPL02},
where a coordinate transformation is used to mold the used phase space volume
into a square.
\index{mapped grid representation}

In some dynamical problems such as the transport of trapped ions, the wave
packet is within in a small region of phase space at every moment, but moves
over large distances (and values of velocity) over time. This situation is
efficiently modeled with a moving grid~\cite{SingerRMP10,FuerstNJP2014}, where
the phase space volume $[x_{\min}(t), x_{\min}(t)+\Delta x] \times [p_{\min}(t),
p_{\min}(t) + \Delta p]$  is moved dynamically along with the wave packet.
\index{moving grid}

The pseudo-spectral basis functions associated with the Fourier grid is not
the only example of a \emph{collocation} method, where the expansion
coefficients of a state are the values of the wave functions $\Psi(x_j)$ for
points $x_j$. However, the Fourier grid is the only choice for which the $x_j$
are equidistant. Generally, the spectral basis functions should be chosen such
that they diagonalize the kinetic operator; e.g.\ for spherical symmetry with
the magnetic quantum number $m=0$, this would be Legendre polynomials.
The general use of spectral methods has an extensive body of work behind
it~\cite{BoydSpectral} and is central to the efficient numerical treatment of
molecular dynamics.

%Approximations that reduce the degrees of freedom, discussed in
%chapter~\ref{chap:transmon}

\section{Simulation of Quantum Dynamics}

Once the application of the Hamiltonian or Liouvillian to a state has been
realized, the next step is to simulate the dynamics of the quantum system. This
usually means solving the time-dependent Schrödinger equation~\eqref{eq:tdse} or
the Liouville-von Neumann equation~\eqref{eq:LvN_dissipator}.

There are two possible approaches to obtaining a solution. The first is to
simply take the equation of motion and apply one of the generic numerical
methods for solving ordinary differential
equations (ODEs), like one of the Runge-Kutta (RK)
methods~\cite{LambertODEBook,NumRecipesFortran}.
\index{Runge-Kutta method}%
This approach has the benefit that ODE solvers such as RK45 are readily
available in numerical libraries. They are also very flexible
with respect to the equation of motion, as long is it is reasonably
well-behaved~\cite{LambertODEBook}. For example, instead of the Schrödinger
equation, the non-linear Gross-Pitaevskii-equation describing a Bose-Einstein
condensate~\cite{LeggettRMP2001}, can easily be solved. However, general ODE
solvers will also lack in numerical efficiency and, most importantly, accuracy.

The alternative approach is to solve the equation of motion analytically, and
then to evaluate that solution numerically. In this way, results of arbitrary
precision can be obtained, with the obvious caveat that the propagation scheme
will be specific to a particular equation of motion and its solution.

The Schrödinger equation for a constant Hamiltonian has the solution
\begin{equation}
  \Ket{\Psi(T)}= \Op{U}(T,0) \Ket{\Psi(0)}
               = \ee^{-\frac{\ii}{\hbar} \Op{H}\, T} \Ket{\Psi(0)}\,,
\end{equation}
For time-dependent Hamiltonians, we approximate $\Op{H}(t)$
as piecewise constant on a time grid with time step $dt$. Then, the total time
evolution operator is the product of the time evolution operators at each time
step,
\begin{equation}
  \Op{U}[T,0]
  = \prod_{i=1}^{nt-1} \underbrace{\Op{U}(t_i+dt, t_i)}_{\equiv \Op{U}_i}
  = \prod_{i=1}^{nt-1} \exp\bigg[-\frac{\ii}{\hbar}
    \underbrace{\Op{H}\left(t_i + \frac{dt}{2}\right)}_{\equiv \Op{H}_i} dt\bigg]\,.
\end{equation}
The time step $dt$ must be chosen sufficiently small that
this is a good approximation; in practice, convergence is checked by verifying
that the numerical results remain stable within a desired precision when $dt$ is
decreased. The number of necessary time steps can be reduced significantly if
a rotating wave approximation (RWA) is justified. As shown in
appendix~\ref{AppendixRWA}, the RWA allows to eliminate fast oscillations in the
pulse, leaving only a slowly varying shape.

A naive way of evaluating $\ee^{-\frac{\ii}{\hbar}\Op{H}_i\,dt}$ is to
diagonalize the Hamiltonian and use the eigendecomposition
\index{eigendecomposition}%
\begin{equation}
  \Op{H}_i = \Op{W}_i
  \begin{pmatrix}
    \lambda_1^{(i)}   & & \\
    & \ddots & \\
    && \lambda_N^{(i)}
  \end{pmatrix}
  \Op{W}_{i}^\dagger,
\end{equation}
where $\Op{W}_i$ contains the eigenvectors of $\Op{H}_i$ as
columns, to write the propagation step as
\begin{equation}
  \ee^{-\frac{\ii}{\hbar} \Op{H}_i dt} \Ket{\Psi}
  = \Op{W}_i \begin{pmatrix}
      \ee^{-\frac{\ii}{\hbar} \lambda_1^{(i)} dt} &&\\ &\ddots&\\
      &&\ee^{-\frac{\ii}{\hbar} \lambda_N^{(i)} dt}
    \end{pmatrix}
    \Op{W}_i^\dagger \Ket{\Psi}
\end{equation}
This ``exact'' exponentiation is suitable for Hilbert spaces of trivially small
dimension $<10$. Since diagonalization scales as $N^3$ with the dimension $N$ of
the matrix~\cite{DemmelSJSC2008}, full diagonalization at every time step
quickly becomes numerically infeasible. Moreover, \Op{H} and \Op{W} must be
constructed as dense matrices.

For a system of non-trivial size, $\exp[-\frac{\ii}{\hbar} \Op{H} dt]$ is
evaluated by expanding the exponential in a polynomial series,
\begin{equation}
  \ee^{-\frac{\ii}{\hbar} \Op{H}\, dt} \Ket{\Psi}
  = \sum_{n=0}^{N-1} a_n P_n(\Op{H}) \Ket{\Psi}\,.
  \label{eq:poly_expansion}
\end{equation}
where $P_n(\Op{H})$ is a polynomial of degree $n$ and $a_n$ is the expansion
coefficient. Applying $P_n(\Op{H})$ to $\ket{\Psi}$ then simply means repeated
applications of $\Op{H}$. For this reason, an efficient propagation relies on
the proper use of sparsity in storing the Hamiltonian and spectral methods such
as the Fourier grid.

The idea of evaluating the exponential as a polynomial series is already
presupposed by the very definition of the exponential of an operator,
\begin{equation}
  \exp[\Op{A}] \equiv \sum_{n=0}^{\infty} \frac{1}{n!} \Op{A}^n\,.
\end{equation}
However, this Taylor expansion converges particularly slowly and is numerically
unstable~\cite{Tal-EzerJCP84}. Thus, it is not suitable for time propagation.
Instead, a polynomial basis must be chosen such that
Eq.~\eqref{eq:poly_expansion} converges quickly and can be
truncated as early as possible.

\subsection{Chebychev Propagation}
\index{Chebychev propagator!homogeneous}%
\label{subsec:chebychev}

For a function $f(x)$ with $x \in [-1, 1] \in \Real$, it can be
shown~\cite{GilBook2007} that the fastest converging polynomial series is the
expansion in Chebychev polynomials
\begin{equation}
  P_n(x) = \cos(n \theta); \qquad \theta = \arccos(x)\,.
\end{equation}
The function $f(x)$ must be sampled at $N$ discrete points $\{x_k\}$ that are
either the roots or the extrema of the $N$'th Chebychev polynomial; the
expansion coefficients are
\begin{equation}
  a_n = \frac{2-\delta_{n0}}{N} \sum_{k=0}^{N-1} f(x_k) P_n(x_k)
\end{equation}

When using the Chebychev expansion for propagation, the requirement that the
argument of $f(x)$ must be real translates into $\Op{H}$ being Hermitian.
Secondly, to account for the requirement that $x \in [-1, 1]$, the Hamiltonian
must be normalized as~\cite{KosloffJCP88, TannorBook, NdongJCP09}
\begin{equation}
  \Op{H}_{\norm} = 2 \frac{\Op{H} - E_{\min}\,\unity}{\Delta} - \unity\,,
\end{equation}
where $\Delta = E_{\max} - E_{\min}$ is the spectral radius and $E_{\max}$ and
$E_{\min}$ are the smallest and largest eigenvalue.

For $f(\Op{H}) = \ee^{-\ii \Op{H}\, dt}$, the series converges for $N$ being
a small multiple of $\lfloor\alpha\rfloor$ with $\alpha = \frac{\Delta}{2}\,dt$.
In this case, the expansion coefficients are can be calculated analytically
as~\cite{Tal-EzerJCP84}
\begin{equation}
  a_n = (2-\delta_{n0})
        \ee^{-\ii \left( \frac{\Delta}{2} + E_{\min}\right)\,dt}
        J_k(\alpha)\,,
\end{equation}
where $J_k(\alpha)$ is the Bessel function of first kind.
In order to calculate the propagated state
\begin{equation}
  \Ket{\Psi}
  = \ee^{-\ii \Op{H} \,dt} \Ket{\Psi_0}
  = \sum_{n} a_n \underbrace{P_n(-\ii \Op{H}_{\norm}) \Ket{\Psi_0}}_{%
                                          \Ket{\Phi_n}}\,.
  \label{eq:cheby_expansion}
\end{equation}
The series is truncated as soon as $\Abs{a_k}$ is below machine precision. Since
both $\Op{H}_{\norm}$ and $\ket{\Psi_0}$ are normalized, this guarantees that
the entire term in the series is below machine precision as well.
Eq.~\eqref{eq:cheby_expansion} is evaluated using the recursive relationship of
the Chebychev polynomials~\cite{KosloffJCP88, TannorBook, NdongJCP09},
\begin{align}
  \Ket{\Phi_0} &= \Ket{\Psi_0}\,, \\
  \Ket{\Phi_1} &= -\ii \Ket{\Phi_0}\,, \\
  \Ket{\Phi_n} &= -2\ii \Ket{\Phi_{n-1}} + \Ket{\phi_{n-2}}\,.
\end{align}
The full algorithm is summarized in appendix~\ref{AppendixAlgos} as
Algorithm~\ref{al:ChebyProp}.

The propagator is stable as long as the spectrum of the Hamiltonian
is in the range $[E_{\min}, E_{\min} + \Delta]$ that is used to obtain
the Chebychev coefficients. Therefore, for propagation on a time grid with
a Hamiltonian of the form $\Op{H} = \Op{H}_0 + \epsilon(t) \Op{H}_1$,
the Chebychev coefficients can be calculated once and then re-used for every
propagation step. A good heuristic in this case is to choose a spectral radius
that includes the spectrum of both
$\Op{H}_{\max} = \Op{H}_0 + \max_t\epsilon(t) \Op{H}_1$, and
$\Op{H}_{\min} = \Op{H}_0 + \min_t\epsilon(t) \Op{H}_1$.

Still, it is essential to have a good approximation for the spectral range of
a given Hamiltonian. Specifically for molecular dynamics, estimating the
spectral range from the extrema of the potentials and the kinetic energy gives
sufficiently accurate results~\cite{Tal-EzerJCP84}. More generally, a very good
approximation of the minimum and maximum eigenvalue of an operator can be
obtained using the \emph{Arnoldi} method.
\index{Arnoldi method}%
Starting from a random state $\ket{\Psi_0}$, the \emph{Krylov space}
\index{Krylov space}%
is built by repeatedly applying \Op{H} and orthonormalizing the obtained
states~\cite{ArnoldiQAM1951, Kosloff96}. The projection of the Hamiltonian into
this Krylov space yields a Hessenberg matrix, whose maximum and minimum
eigenvalues converge towards the maximum and minimum eigenvalues of the
Hamiltonian. The algorithm is detailed in appendix~\ref{AppendixAlgos}. For
each iteration $j$, the spectral range is calculated from the eigenvalues
obtained in line~\ref{Ar:eigenvalues} of Algorithm~\ref{al:Arnoldi}. The
iteration continues until the result is converged to some predefined precision.


The Chebychev method can also be applied to other equations of
motion, as long as an analytical solution can be derived and expanded in
Chebychev polynomials. The expansion coefficients in this case cannot be
expressed in Bessel functions, but must be derived using a cosine transform, as
outlined in appendix~\ref{AppendixFFT}. In this way, a Chebychev propagator
for the inhomogeneous Schrödinger equation has been derived~\cite{NdongJCP09}.
\index{Chebychev propagator!inhomogeneous}

For a true time-dependent Hamiltonian that is not well-approximated as
piecewise constant the formal solution of the Schrödinger
equation~\eqref{eq:tdse} is $\ket{\Psi(t)} = \Op{U}[t,0] \ket{\Psi(0)}$, with
\index{time evolution operator}%
\begin{equation}
  \Op{U}[t,0]
  = \Timeorder \exp\left[
      - \frac{\ii}{\hbar} \int_{0}^{t} \Op{H}(t') \dd t'
    \right]\,,
\end{equation}
where $\Timeorder$ represents the time ordering operator.
\index{time ordering operator}%
This can be rewritten as the solution to an inhomogeneous Schrödinger
equation~\cite{NdongJCP10}, and thus also be evaluated using the inhomogeneous
Chebychev propagator.

\enlargethispage{\baselineskip}
In principle, the solution to the Liouville-von Neumann
equation~\eqref{eq:LvN_dissipator} take the same form as the time evolution
operator in Hilbert space, i.e.\
\begin{equation}
  \Op{\rho}(t) = \ee^{- \frac{\ii}{\hbar} \Liouville t} \Op{\rho}(0)\,.
\end{equation}
However, unlike the Hamiltonian, the Liouvillian is generally not a Hermitian
operator, except when there is no dissipator, i.e.\ $\Liouville[\Op{\rho}]
= [\Op{H}, \Op{\rho}]$. In this case, the spectrum of
$\Liouville$ is in the range $[-\Delta,+\Delta]$, where $\Delta$ is the spectral
radius of $\Op{H}$.

\subsection{Newton Propagation}
\label{subsec:newton}

In the general case of a dissipative Liouvillian, instead of a Chebychev
expansion, an expansion in Newton polynomials can be used.
\index{Newton propagator}%
For a general function $f(z)$ with $z \in \Complex$, the expansion in Newton
polynomials $R_n(z)$ reads
\begin{equation}
  f(z) \approx \sum_{n=0}^{N-1} a_n R_n(z); \quad
  R_n(z) = \prod_{j=0}^{n-1} \left( z-z_j \right)\,,
\end{equation}
for a set of sampling points $\{z_j\}$ at which the interpolation is exact.
The coefficients are defined as the \emph{divided
differences}~\cite{AshkenaziJCP95}
\index{divided differences}%
\begin{subequations}
\label{eq:divided_differences}
\begin{align}
  a_0 &= f(z_0)\,, \\
  a_1 &= f(z_1) - f(z_0)\,, \\
  a_n &= \frac{f(z_n) - \sum_{j=0}^{n-1} a_j
               \prod_{k=0}^{j-1}\left(z_n - z_k\right)}
              {\prod_{j=0}^{n-1} \left(z_n - z_j\right)}\,.
\end{align}
\end{subequations}
For solving the Liouville-von Neumann equation,
$f(z)=\ee^{-\ii z \,dt}$, where the argument $z$ is the Liouvillian $\Liouville$.
Thus, the propagation is written as
\begin{equation}
  \Op{\rho} = \ee^{-\ii \Liouville \,dt} \, \Op{\rho}_0
            \approx
              \underbrace{%
                \sum_{n=0}^{N-1} a_n
                \left( \Liouville - z_j\unity \right) }_{%
                          \equiv p_{N-1}(\Liouville)}
                \Op{\rho}_0\,,
\end{equation}
where the polynomial is evaluated through repeated application of
Eq.~\eqref{eq:LvN_dissipator}.

The central issue for obtaining a fast-converging series is a proper choice of
the sampling points $\{z_j\}$. The fastest convergence results from using the
complex eigenvalues of $\Liouville$~\cite{KosloffARPC94}. However, the exact
eigenvalues of the Liouvillian are not readily available. More generally,
arbitrary points from the spectral domain of $\Liouville$ can be used as
sampling points.

A widely used method is to estimate the spectral domain and to
encircle it with a rectangle or ellipse~\cite{BermanJPA92, AshkenaziJCP95,
HuisingaJCP99}. Then, a large number of expansion coefficients are calculated
from sampling points on that boundary. The same coefficients are used for the
propagation of all Liouvillians on the time grid under the assumption that the
all fit into the same encirclement. The series is truncated as soon as
convergence is reached. This is similar to the method employed for the
Chebychev propagator, where a set of coefficients is calculated once and then
used for the propagation of any Hamiltonian that is within the same spectral
range.

A middle path between the exact eigenvalues of $\Liouville$ and the crude
encirclement of the spectral domain is the use of the Krylov method to obtain
approximate eigenvalues. The same method was already employed to estimate the
spectral radius for the Chebychev propagator. The Arnoldi
algorithm~\ref{al:Arnoldi} in appendix~\ref{AppendixAlgos} for $\hat{A}
= \Liouville$ and  using $\vec{v} = \Op{\rho}$ as a starting vector yields a set
of approximate eigenvalues of $\Liouville$, as well as a Hessenberg matrix
$\hat{H}$ that is the projection of $\Liouville$ in the Krylov subspace, and the
set of Arnoldi vectors that span that subspace. Instead of using $\Liouville$
as the arguments of the polynomial $p_{N-1}$, the Hessenberg matrix may be used.
If $\hat{V}_{N}$ is the transformation matrix between the full Liouville space
and the reduced Krylov space, consisting of the Arnoldi vectors as columns, the
propagation is evaluated using
\begin{equation}
  \Liouville \Op{\rho}
  \approx
  \hat{V}_{N}\, p_{m-1}(\hat{H}) \, \hat{V}_{N}^{\dagger} \Op{\rho}_0\,.
\end{equation}
Assuming $N$ is much smaller than the full dimension of the
Liouville space, most of the numerical effort is in the Arnoldi algorithm, in
constructing the Krylov space.

However, even for moderate values of $N$ (typically on the order of 100), the
Arnoldi algorithm can require prohibitive amounts of memory. This is because
a full set of $N$ Arnoldi vectors, each of the dimension of the Liouville space,
need to be stored. To counter this problem, a iterative scheme has been
developed~\cite{Tal-EzerSJSC2007}. Instead of performing the Arnoldi
algorithm to a high order $N$, until convergence is reached in the propagation,
we stop at some small order $m<10$. This gives a first approximation to the
propagated density matrix,
\begin{equation}
  \Op{\rho}^{(1)}
  = p_{m-1}^{(0)}(\Liouville) \Op{\rho}_0
  = \sum_{n=0}^{m-1} a_n R_n(\Liouville) \Op{\rho}_0\,.
  \label{eq:newton1stIter}
\end{equation}
%with
%\begin{equation}
%  \Liouville^{(0)} \equiv
%  \hat{V}_{m-1}^{(0)} \hat{H}^{(0)}
%    \hat{V}_{m-1}^{(0)\,\dagger}\,.
%\end{equation}
The idea is now to iteratively add remaining terms to the Newton series in
chunks of size $m$, retaining all coefficients and sampling points, but
restarting the Arnoldi procedure in every iteration.

Adding the next $m$ terms to Eq.~\eqref{eq:newton1stIter} yields
\begin{equation}
\begin{split}
  \Op{\rho}^{(2)}
 &= \Op{\rho}^{(1)} + \sum_{n=m}^{2m-1} a_n R_n(\Liouville) \Op{\rho}_{0} \\
 &= \Op{\rho}^{(1)}
    + \underbrace{\left(\sum_{n=0}^{m-1}  a_{m+n} R_n^{(1)}(\Liouville)\right)}_{%
                               \equiv p_{m-1}^{(1)} }
      \underbrace{\left(R_{m}^{(0)}(\Liouville) \Op{\rho}_{0} \right)}_{%
                           \equiv \Op{\sigma}^{(1)} }\,,
\end{split}
\end{equation}
with
\begin{equation}
  R_n^{(0)}(\Liouville) = \prod_{j=0}^{n-1}(\Liouville - z_{j}\unity), \qquad
  R_n^{(1)}(\Liouville) = \prod_{j=0}^{n-1}(\Liouville - z_{n+j}\unity)\,.
\end{equation}
That is, the terms in $R_n(\Liouville)$ already known from the calculation of
$\Op{\rho}^{(1)}$ have been pulled out, and yield a new ``starting vector''
$\Op{\sigma}^{(1)}$, which is the argument to a Newton series of only $m$ new
terms. The new sampling points on which the $R_n^{(1)}$ are evaluated are
obtained by applying the Arnoldi procedure to $\Op{\sigma}^{(1)}$. The Newton
coefficients continue recursively from the previous restart.
The third iteration yields
\begin{equation}
  \Op{\rho}^{(3)}
  = \Op{\rho}^{(2)}
    + \underbrace{\left(\sum_{n=0}^{m-1}  a_{2m+n} R_n^{(2)}(\Liouville)\right)}_{%
                               \equiv p_{m-1}^{(2)} }
      \underbrace{\left(R_{m}^{(1)}(\Liouville) \Op{\sigma}_{1} \right)}_{%
                           \equiv \Op{\sigma}^{(2)} }\,.
\end{equation}
The Newton propagator continues, adding the $m$ terms
evaluating
\begin{equation}
  p_{m-1}^{(s)}(\Liouville) \Op{\sigma}^{(s)}
  = \sum_{n=0}^{m-1} a_{sm + n}
    \prod_{k=0}^{n-1} \left(\Liouville - z_{sm+k} \unity \right)
    \Op{\sigma}^{(s)}
\end{equation}
with
\begin{equation}
  \Op{\sigma}^{(s)} = p_{m-1}^{(s-1)} \Op{\sigma}^{(s-1)}
\end{equation}
at every restart iteration. The complete algorithm is listed in
appendix~\ref{AppendixAlgos}.

In the implementation of the algorithm, there are two details that need to be
taken into account for numerical stability. First, the denominator of the
divided differences in Eq.~\eqref{eq:divided_differences} may become extremely
small if consecutive sampling points are close to each other. This can be
addressed by reordering the points such that the denominator in the divided
differences is maximized. This process is called \emph{Leja
ordering}~\cite{ReichelBIT1990}.
\index{Leja ordering}%
The reverse problem that the sampling points
are too far apart, causing an underflow in the calculation of coefficients can
be avoided by normalizing the Liouvillian as
\begin{equation}
  \tilde{\Liouville} = \frac{1}{\rho} \left( \Liouville - c \right)\,,
\end{equation}
where $c$ is an estimate for the center of the spectrum of $\Liouville$, and the
eigenvalue are roughly contained in a radius $\rho$ around $c$. These values can
be estimated from the sampling points obtained in the first iteration of the
Newton propagator. The normalization of the Liouvillian is in some sense similar
to the normalization of the Hamiltonian in the Chebychev propagator, but there
it is crucial since the Chebychev polynomials are only defined in the domain
$[-1, 1]$. For the Newton propagator, the normalization is only for numerical
stability.

The primary use for the Newton propagator is to solve the Liouville-von Neumann
equation with a dissipative term. However, it can also be used to solve the
Schrödinger equation with a non-Hermitian Hamiltonian. This is sometimes used as
an ad-hoc model for spontaneous decay, avoiding the significant overhead
incurred by modeling the system properly in Liouville space. As seen in
section~\ref{sec:decay_and_dephasing} of chapter~\ref{chap:quantum}, spontaneous
decay from a level $\ket{n}$ results in the population of that level decreasing
proportionally to $\ee^{-\gamma_1} t$. To obtain the same decay behavior in
Hilbert space, the complex amplitude $a_n$ of the level $\ket{n}$ must decay
at a rate of $\frac{\gamma_1}{2}$, since the population of $\ket{n}$ is given by
$\Abs{a_n}^2$. This is achieved by adding a non-Hermitian term to the
Hamiltonian, resulting in
\begin{equation}
  \Op{H}_{\gamma} = \Op{H} - \ii \frac{\gamma_1}{2} \KetBra{n}{n}\,.
\end{equation}
Propagation with such a Hamiltonian does not conserve the norm of $\ket{\Psi}$;
the population that has decayed simply vanishes. Specifically, it is not added
to the state to which $\ket{n}$ decays, since the incoherent superposition of
state cannot be modeled in Hilbert space. Nonetheless, a non-Hermitian
Hamiltonian can be a useful way to determine e.g.\ how much the fidelity of
a quantum operation is affected if spontaneous decay is taken into account.


\section{Optimization Methods}

\subsection{Optimization of Two-Qubit Quantum Gates}

Building upon the ability to simulate the dynamics of a quantum system, the
final step is to apply the methods of numerical optimal control (OCT), as
introduced in chapter~\ref{chap:intro}. The focus in this thesis is on the
realization of two-qubit quantum gates $\Op{O}$.

For a quantum gate to be successfully implemented at time $T$, all basis states
of the two-qubit logical subspace must evolve according to
\begin{equation}
  \Op{U}(0,T) \Ket{k} \stackrel{!}{=} \ee^{\ii \phi} \Op{O} \Ket{k}\,,
  \qquad
  \Ket{k} \in \{ \Ket{00}, \Ket{01}, \Ket{10}, \Ket{11} \}\,,
  \label{eq:gate_target_hilbert}
\end{equation}
with an arbitrary global phase $\phi$.
Likewise, in Liouville space,
\begin{equation}
  V(T)\left[\Op{\rho}_{ij}\right]
  \stackrel{!}{=}
  \Op{O} \Op{\rho}_{ij} \Op{O}^\dagger\,,
  \qquad
  \Op{\rho}_{ij} = \KetBra{i}{j}\,,
\end{equation}
where $V(T)[\Op{\rho}]$ denotes the dynamical map.

The most straightforward way to express this in a functional is in terms of the
complex overlaps
\begin{equation}
  \label{eq:tau_hilbert}
  \tau_k = \Braket{k | \Op{O}^\dagger \Op{U}(T,0) | k}
\end{equation}
in Hilbert space, or
\begin{equation}
  \label{eq:tau_liouville}
  \tau_k = \Tr\left[\left(\Op{O}^\dagger \Op{\rho}_k^{\dagger} \Op{O}\right)
                    V(T)\left[\Op{\rho}_k\right]
           \right]
\end{equation}
in Liouville space.
Two commonly used possibilities for obtaining a real-valued functional
are~\cite{PalaoPRA03}
\label{eq:pk_functionals}
\begin{align}
  \JTsm &= 1- \frac{1}{N^2} \Abs{\sum_{k=1}^{N} \tau_k}^2
         = 1- \frac{1}{N^2} \sum_{k=1}^{N} \sum_{l=1}^{N} \tau_{l}^* \tau_{k}\,,
  \label{eq:JTsm}\\
  \JTre &= 1-\frac{1}{N} \Re \left[\, \sum_{k=1}^{N} \tau_k \,\right]
         = 1-\frac{1}{N} \sum_{k=1}^{N} \frac{1}{2} \left( \tau_k + \tau_k^* \right)
  \label{eq:JTre}\,,
\end{align}
where $N=4$ in Hilbert space and $N=16$ in Liouville space. $\JTsm$ takes its
minimum value of 0 if
Eq.~\eqref{eq:gate_target_hilbert} is fulfilled with an arbitrary phase $\phi$,
whereas $\JTre$ is 0 only for a global phase of $\phi=0$. Note that in Hilbert
space,
\begin{equation}
  \sum_{k=1}^{N} \tau_k = \Tr[\Op{O}^\dagger \Op{U}]
  \label{eq:tr_Odg_U}
\end{equation}
is the Hilbert-Schmidt overlap of the target gate with the implemented gate.
In Liouville space, $\JTsm$ and $\JTre$ are equivalent, as global phases cannot
be expressed; $\JTre$ is preferred due to its simpler form.

It is important to note that $\JTre$ or $\JTsm$ are used simply as optimization
functionals. While in Hilbert space, $\JTsm$ still has a well-defined meaning
(the probability that a state $\Op{U} \ket{\Psi}$ is obtained in a measurement
of $\Op{O} \ket{\Psi}$), this is no longer the case in Liouville space. In
general, a physically meaningful figure of merit of the success of implementing
a quantum gate is through the average gate fidelity $F_{\avg}$ defined in
Eq.~\eqref{eq:Favg}. For optimization purposes, we have the freedom to choose
an alternative figure of merit $J_T$ with a simpler structure, as long as both
$J_T$ and the ``physical'' gate error $1-F_{\avg}$ reach their minimum value for
an optimal pulse, and only return to $F_{\avg}$ afterwards to compare results.

The Hamiltonian that induces $\Op{U}(T,0)$ or $V(T)$ takes the form
$\Op{H} = \Op{H}_d + \Op{H}_c(\epsilon(t))$,
with the drift Hamiltonian $\Op{H}_d$ and a control Hamiltonians
$\Op{H}_c[\epsilon(t)]$ that depends on a driving field $\epsilon(t)$,
parametrized through an arbitrary number of
independent control parameters $\epsilon_j$. In most cases
considered here, the pulse is approximated as piecewise constant on
a time grid, and the control parameters are simply the values of $\epsilon_j \equiv
\epsilon(t_j + \frac{dt}{2})$. A more constrained parametrization with fewer
control parameters would be provided through predefined analytical shapes,
e.g.\ for a Gauss-shaped pulse, the $\epsilon_j$ would be the amplitude,
duration, and central frequency; for a spectral decomposition, the $\epsilon_j$
would be the amplitudes for the frequencies $\omega_j$.

The task is to minimize a functional such as $\JTsm$ or $\JTre$, in an iterative
procedure: in each OCT iteration $i$, we find an update to the pulse
$\epsilon^{(i)}(t)$ such the updated pulse $\epsilon^{(i+1)}(t)$ yields an
improved value for the functional.
The optimization loop continuous until $J$ reaches a value that is smaller than
some predefined limit, or until the value of $J$ shows no significant
improvement.

There are two basic categories of algorithms for finding updates for the control
parameters that improve the value of the functional. The first are
``gradient-free'', employing only evaluations of the functional.
\index{optimization!gradient-free}%
Most prominently, this includes the downhill-simplex algorithm discussed in
section~\ref{subsec:simplex}.  Gradient-free optimization algorithms are
extremely versatile and easy to apply, but they also tend to converge very
slowly, particularly for a large number of control parameters. Moreover, they
are prone to running into local minimal, although more advanced \emph{global}
methods such as genetic algorithms~\cite{GoldbergGABook1989} and swarm
search optimization~\cite{KennedySwarm1995} also exist.
\index{optimization!global}%
Gradient-free optimization methods can easily be incorporated in experimental
setups as a \emph{closed-loop} control, where a measurement determines the
\index{closed-loop control}%
figure of merit and drives a variation of the control parameters for the next
iteration.

The second category are gradient-based methods.
\index{optimization!gradient-based}%
Including information about how the optimization functional varies with changes
in the controls greatly speeds up convergence. However, it requires to derive
analytical expressions for the gradient, and then additional numerical resources
for evaluating that gradient. A \emph{concurrent} scheme like the gradient
ascent discussed in section~\ref{subsec:gradient_ascent} varies each control
parameter individually according to the derivative of the optimization
functional with respect to that parameter. In contrast, Krotov's method
presented in section~\ref{subsec:Krotov} takes the time-continuous $\epsilon(t)$
as a whole. Subsequent discretization to a time grid yields a \emph{sequential}
scheme in which the update for $\epsilon_j$ takes into account the updates at
earlier times. While not applicable to arbitrary pulse
parametrizations, Krotov's method guarantees monotonic convergence.
Gradient-based methods are typically used as a \emph{open-loop} control, where
\index{open-loop control}%
the entire optimization is performed based on a numerical simulation, before
taking the final optimized set of controls to an experimental implementation.

For simplicity, we have only considered a single control Hamiltonian and driving
field here; in general, there may be multiple control Hamiltonians and driving
fields.  In this case, each driving field is parametrized independently.

\subsection{Downhill Simplex Optimization}
\label{subsec:simplex}
\index{simplex optimization}

The downhill simplex, or Nelder-Mead algorithm~\cite{NelderCJ1965} is a
\index{Nelder-Mead algorithm}%
particularly simple, gradient-free optimization method that is very effective
if there is only a handful of $N$ optimization parameters. The idea is to
construct a simplex polytope consisting of $N+1$ points in the $N$-dimensional
parameter space. The point with the largest value of the functional is then
replaced by reflection on the remainder of the polytope, followed by some
contraction and expansion steps, yielding a new point with an improved value of
the functional. Intuitively, the simplex ``rolls'' down the optimization
landscape.

The fact that the algorithm only relies on the \emph{evaluation} of the
optimization functional makes it extremely versatile, giving a black box method
for the optimization of arbitrary figures of merit. It is well-suited as
a pre-optimization for finding pulses of simple analytical forms that may then
be optimized further on using a gradient-based method on a time grid. In
particular, the pulse duration $T$ can easily be included as a control parameter
for a simplex method. In contrast, $T$ must be fixed when optimizing on a time
grid.

Non-gradient method may be used for controls that are inherently coarse-grained,
due to the limitations of experimental setups, e.g.\ the limited number of
pixels in early femtosecond pulse shapers~\cite{WeinerRSI2000}. Some control
problems, specifically in quantum many-body systems~\cite{DoriaPRL11} have been
demonstrated to have solutions that contain only a small number of
frequency components.
The CRAB algorithm~\cite{CanevaPRA2011} (Chopped RAndom Basis) has been developed
for this class of problems;
\index{CRAB algorithm}%
it uses a pre-specified number of $N$ frequency components, but chooses
those frequencies at random. That is, the control pulse is parametrized as
\begin{equation}
  \epsilon_{\text{CRAB}}(t)
  = S(t) \sum_{n=1}^{N} \left(
    a_n \sin(\omega_n t) + b_n \cos(\omega_n t)
  \right)\,,
\end{equation}
with a pulse shape $S(t)$. The frequencies are chosen as
\begin{equation}
  \omega_n = \frac{2 \pi n}{T} (1+r_n)\,; \qquad r_n \in [-0.5, 0.5]\,,
\end{equation}
where $T$ is the pulse duration and the $r_n$ are random numbers.
The coefficients $a_n$ and $b_n$ are optimized using the downhill-simplex
algorithm, with many optimizations running in parallel, using different
randomized frequencies. The CRAB method can be useful since it limits the
effective number of control parameters without assuming a simple analytical
pulse shape, but it fails for control problems whose solution require a large
number of spectral components.

\subsection{Gradient Ascent}
\label{subsec:gradient_ascent}

The GRAPE algorithm~\cite{KhanejaJMR05} (Gradient Ascent Pulse Engineering)
\index{GRAPE algorithm}%
considers the gradient $\partdifquo[J]{\epsilon_j}$ with respect to any
control parameter and then updates that control parameter according to
\begin{equation}
  \epsilon_j^{(i+1)}
  = \epsilon_j^{(i)} - \alpha \partdifquo[J]{\epsilon_j}\,,
\end{equation}
using a suitable step width $\alpha$.

For functionals depending on the overlap between propagated and expected state,
and assuming a time-grid parametrization, the gradient of
Eq.~\eqref{eq:tau_hilbert} becomes
\begin{equation}
\begin{split}
  \partdifquo[\tau_k]{\epsilon_j}
  &= \partdifquo{\epsilon_j}
    \Braket{k | \Op{O}^\dagger
            \Op{U}_{nt-1} \dots \Op{U}_{j} \dots \Op{U}_{1} | k} \\
  &=
    \Braket{k | \Op{O}^\dagger\,
                \Op{U}_{nt-1} \dots \Op{U}_{j+1}
                \, \partdifquo[\Op{U}_{j}]{\epsilon_j} \,
                \Op{U}_{j-1} \dots \Op{U}_{1} | k} \\
  &=
    \Braket{\chi_k(t_{j+1}) | \partdifquo[\Op{U}_{j}]{\epsilon_j} | \phi_k(t_j)}\,,
  \label{eq:grape_gradient}
\end{split}
\end{equation}
where $\ket{\phi_k(t)}$ is the forward-propagated basis state $\ket{k}$, and
$\ket{\chi(t)}$ is the backward-propagated target state $\Op{O}\ket{k}$.
The numerical effort in calculating the gradient compared to a simple evaluation
of the functional is therefore an additional backward propagation. Moreover, the
states of either the backward or the forward propagation at every point in time
needs to be stored in order to calculate the gradient.
The derivative of the $j$'th time evolution operator is given by
\begin{equation}
\partdifquo[\Op{U}_{j}]{\epsilon_{j}}
= \partdifquo{\epsilon_{j}} e^{-\frac{\ii}{\hbar} \Op{H}_j dt}
\equiv
  \sum_{n=1}^{\infty} \frac{(-\ii\,dt)^n}{n!}
  \sum_{k=0}^{n-1} \Op{H}_j^{k}
                  \left(\partdifquo[\Op{H}_j]{\epsilon_j}\right)
                  \Op{H}_j^{n-k-1}\,.
\label{eq:deriv_U_j_taylor}
\end{equation}
In Liouville space, $\Op{H}$ is replaced by $\Liouville$.
For the two functionals $\JTsm$ and $\JTre$ of Eq.~\eqref{eq:JTsm} and
Eq.~\eqref{eq:JTre}, the total gradient is
\begin{align}
  \partdifquo[\JTsm]{\epsilon_j}
  &= - \frac{1}{N^2} \sum_{k=1}^{N} \sum_{l=1}^{N} \left[
          \partdifquo[\tau_l^*]{\epsilon_j} \tau_k
        + \tau_l^* \partdifquo[\tau_k]{\epsilon_j}
     \right]
   = - \frac{2}{N}  \Re \sum_{k=1}^{N} \sum_{l=1}^{N}
        \tau_l^* \partdifquo[\tau_k]{\epsilon_j}\,,
   \\
  \partdifquo[\JTre]{\epsilon_j}
  &= - \frac{1}{N} \sum_{k=1}^{N} \frac{1}{2} \left(
        \partdifquo[\tau_k]{\epsilon_j} + \partdifquo[\tau_k^*]{\epsilon_j}
     \right)
   = - \frac{1}{N} \Re \sum_{k=1}^{N} \partdifquo[\tau_k]{\epsilon_j}\,.
\end{align}

Using only the gradient of the functional to steer the optimization will
generally not yield sufficiently fast convergence; the situation could be
improved by using Newton's method, taking into account the second derivative,
i.e.\ the Hessian
\begin{equation}
  H_{jj'} = \frac{\partial^2 J}{\partial \epsilon_j \partial \epsilon_{j'}}.
\end{equation}
However, calculating the Hessian is generally prohibitively expensive.
Therefore, quasi-Newton methods are employed~\cite{MachnesPRA2011}.
\index{quasi-Newton method}%
The idea is to estimate the Hessian using only gradient information.
There are several approaches of how to achieve this, the most popular
one~\cite{NocedalBook} is the BFGS
\index{BFGS}%
method~\cite{BroydenIMAJAM1970,FletcherCJ1970,GoldfarbMC1970,ShannoMC1970}.
A memory-efficient version of this method that also allows to define bounds on
the control, named LBFGS-B~\cite{Byrd94} is widely available as a Fortran
library~\cite{ZhuATMS97}.
\index{LBFGS-B}

It is crucial for the application the LBFGS-B method that the gradient is
calculated to full machine precision. The Taylor series in
Eq.~\eqref{eq:deriv_U_j_taylor} would usually have to be evaluated to very high
order. A more efficient and stable method for calculating the gradient is
provided by the observation that~\cite{FouquieresJMR2011}
\begin{equation}
\begin{split}
  \exp\left[ -\frac{\ii}{\hbar} \begin{pmatrix}
    \Op{H}_j  & \partdifquo[\Op{H}_j]{\epsilon_j} \\
    0         & \Op{H}_j
  \end{pmatrix} \, dt \right]
  \begin{pmatrix}
    0 \\ \Ket{\Psi}
  \end{pmatrix}
  &
  = \begin{pmatrix}
    \Op{U}_j   & \partdifquo[\Op{U}_j]{\epsilon_j} \\
    0          & \Op{U}_j
  \end{pmatrix}
  \begin{pmatrix}
    0 \\ \Ket{\Psi}
  \end{pmatrix}
 \\ &
 =
  \begin{pmatrix}
    \partdifquo[\Op{U}_j]{\epsilon_j} \Ket{\Psi} \\ \Op{U}_j \Ket{\Psi}
  \end{pmatrix}
\end{split}
\label{eq:schirmer_gradient}
\end{equation}
Eq.~\eqref{eq:schirmer_gradient} can be efficiently evaluated by applying the
Newton propagator of appendix~\ref{AppendixAlgos}.

\subsection{Krotov's Method}
\label{subsec:Krotov}

For time-continuous controls, Krotov's method~\cite{KonnovARC99}
\index{Krotov's method}%
considers a functional of the form
\begin{equation}
  J[\epsilon^{(i)}(t)]
  = J_T(\{\phi_k^{(i)}(T)\})
      + \int_0^T g_a[\epsilon^{(i)}(t)] \dd t
      + \int_0^T g_b[\{\phi^{(i)}_k(t)\}] \dd t\,.
\label{eq:J_krotov}
\end{equation}
In addition to the final time functional, e.g.\ $\JTsm$ or $\JTre$ defined in
Eq.~\eqref{eq:JTsm} and Eq.~\eqref{eq:JTre}, \emph{running costs}
that depend on the control field and the propagated states at each point in time
are also included. As before, for a gate optimization, the
$\{\ket{\phi^{(i)}_k(t)}\}$ are the basis states $\{\ket{k}\}$ propagated under the
pulse $\epsilon^{(i)}(t)$ at the current OCT iteration $i$.

Krotov's method uses an auxiliary functional to disentangle the
interdependence of the states and the field, allowing to find an updated
$\epsilon^{(i+1)}(t)$ such that $J[\epsilon^{(i+1)}]  < J[\epsilon^{(i)}]$ is
guaranteed.
The derivation, see Ref.~\cite{ReichJCP12}, yields the condition
\begin{equation}
\begin{split}
  \krotovdifquo[g_a]{\epsilon}{\epsilon^{(i+1)}(t)}
  & =
  2 \Im \left[
    \sum_{k=1}^{N}
    \Bigg\langle
      \chi_k^{(i)}(t)
    \Bigg\vert
      \Bigg(\dHdEps \Bigg)
    \Bigg\vert
      \phi_k^{(i+1)}(t)
    \Bigg\rangle
 + \right. \\ & \qquad \left.
    + \frac{1}{2} \sigma(t)
    \Bigg\langle
      \Delta\phi_k(t)
    \Bigg\vert
      \Bigg(\dHdEps\Bigg)
    \Bigg\vert
      \phi_k^{(i+1)}(t)
    \Bigg\rangle
  \right]\,,
\end{split}
\label{eq:krotov_proto_update}
\end{equation}
\index{Krotov!second order update equation}%
with
$\ket{\Delta \phi_k(t)} \equiv \ket{\phi_k^{(i+1)}(t)} - \ket{\phi_k^{(i)}(t)}$.
Assuming the equation of motion for the forward propagation of
$\ket{\phi_k(0)} = \ket{k}$ is written as
\begin{equation}
  \partdifquo{t} \Ket{\phi_k^{(i+1)}(t)}
  = -\frac{\ii}{\hbar} \Op{H}^{(i+1)} \Ket{\phi_k^{(i+1)}(t)}\,,
 \label{eq:fw_eqm}
\end{equation}
the co-states $\Ket{\chi_k}$ are backward-propagated under the old pulse as
\begin{equation}
  \partdifquo{t} \Ket{\chi_k^{(i)}(t)}
  = -\frac{\ii}{\hbar} \Op{H}^{\dagger\,(i)} \Ket{\chi_k^{(i)}(t)}
    + \krotovdifquo[g_b]{\Bra{\phi_k}}{\phi^{(i)}(t)}\,,
 \label{eq:bw_eqm}
\end{equation}
with the boundary condition
\begin{equation}
  \Ket{\chi_k^{(i)}(T)}
   = - \krotovdifquo[J_T]{\Bra{\phi_k}}{\phi_k^{(i)}(T)}\,.
  \label{eq:chi_boundary}
\end{equation}
\index{Krotov!boundary condition}%
In Eq.~\eqref{eq:krotov_proto_update}, $\sigma(t)$ is a scalar function that
must be properly chosen to ensure monotonic convergence. In many cases, it is
sufficient to set $\sigma(t) \equiv 0$, in
particular if the equation of motioned is linear ($\Op{H}$ does not
depend on $\ket{\phi_k(t)}$), the functional $J_T$ is convex, and no
state-dependent constraints are used ($g_b\equiv 0$). Even for some types of
state-dependent constraints $\sigma(t)$ may be set to zero, specifically for
keeping the population in an allowed subspace~\cite{PalaoPRA08}. However,
a state-dependent constraint adds an inhomogeneity to the equation of motion for
$\ket{\chi_k(t)}$.

Where the second order is required, $\sigma(t)$ can be determined numerically as
shown in Ref.~\cite{ReichJCP12}. In chapter~\ref{chap:pe}, final-time functionals
that depend higher than quadratically on the states are considered, while the
equation of motion remains the linear Schrödinger equation. In this case,
\begin{equation}
  \sigma(t) \equiv -\max\left(\varepsilon_A,2A+\varepsilon_A\right)\,,
  \label{eq:sigma_A}
\end{equation}
where $\varepsilon_A$ is a small non-negative number that can be used to
enforce strict inequality in the second order optimality condition.
The optimal value for $A$ in each iteration can be determined numerically
as~\cite{ReichJCP12}
\begin{equation}
  A  =
  \frac{2 \sum_{k=1}^{N} \Re\left[\Braket{\chi_k(T)|\Delta\phi_k(T)}\right]
        + \Delta J_T}
       {\sum_{k=1}^{N} \Abs{\Delta\phi_k(T)}^2}
  \,,
  \label{eq:A_n}
\end{equation}
with
\begin{equation}
  \Delta J_T \equiv J_T(\{\phi_k^{(i+1)}(T)\}) -J_T(\{\phi_k^{(i)}(T)\})\,.
\end{equation}

In order to obtain an explicit equation for $\epsilon^{(i+1)}(t)$,
a state-dependent running cost $g_a$ must be used, and usually takes the form
\begin{equation}
  g_a[\epsilon(t)]
  = \frac{\lambda_a}{S(t)} \left(\epsilon(t) - \epsRef(t)\right)^2
  \label{eq:g_a_ref}
\end{equation}
with a scaling parameter $\lambda_a$ and a shape function $S(t) \in [0,1]$.
When $\epsRef$ is set to the optimized field $\epsilon^{(i)}$ from the previous
iteration,
\begin{equation}
  g_a[\epsilon^{(i+1)}(t)]
  = \frac{\lambda_a}{S(t)} \left(\Delta\epsilon(t)\right)^2\,,
  \quad
  \Delta\epsilon(t) \equiv \epsilon^{(i+1)}(t) - \epsilon^{(i)}(t)\,,
  \label{eq:g_a_delta}
\end{equation}
and for $\sigma(t) \equiv 0$, the explicit first order Krotov update
equation is obtained~\cite{SklarzPRA02, PalaoPRA03},
\begin{equation}
  \Delta\epsilon(t)
    =
  \frac{S(t)}{\lambda_a} \Im \left[
    \sum_{k=1}^{N}
    \Bigg\langle
      \chi_k^{(i)}(t)
    \Bigg\vert
      \Bigg(\dHdEps \Bigg)
    \Bigg\vert
      \phi_k^{(i+1)}(t)
    \Bigg\rangle
  \right]\,.
  \label{eq:krotov_first_order_update}
\end{equation}
\index{Krotov!first order update equation}%
If $S(t) \in [0,1]$ is chosen as
a function that smoothly goes to zero at $t=0$ and $t=T$, then the update
will be suppressed there, and thus a smooth switch-on and switch-off can be
maintained.
The scaling factor $\lambda_a$ controls the overall magnitude
of the pulse update. Values that are too large will change $\epsilon^{(i)}(t)$
by only a small amount, causing slow convergence.

If the reference field $\epsRef$ is set to zero, such that
Eq.~\eqref{eq:g_a_ref} is a penalty on the pulse fluence, the update equation
turns into a simple replacement, where $\epsilon^{(i+1)}(t)$ is directly given
by the right hand side of Eq.~\eqref{eq:krotov_first_order_update}. However, the
lack of an explicit dependence on the previous field leads to numerical
instability. A better approach for penalizing large pulse amplitudes is to
use the state-dependent running cost
\begin{equation}
  g_a[\epsilon^{(i+1)}(t)]
  = \frac{\lambda_a}{S(t)} \left(\epsilon^{(i+1)}(t) - \epsilon^{(i)}(t)\right)^2
   + \frac{\lambda_\epsilon}{S(t)} \left(\epsilon^{(i+1)}(t)\right)^2\,,
\end{equation}
which leads to
\begin{equation}
  \epsilon^{(i+1)}(t)
  = \frac{\lambda_{a}}{\lambda_{a} + \lambda_{\epsilon}} \epsilon^{(i)}(t)
    + \frac{S(t)}{\lambda_{a} + \lambda_{\epsilon}}
    \frac{\delta\epsilon(t)}{2}\,,
\end{equation}
where $\delta\epsilon(t)$ is the right hand side of
Eq.~\eqref{eq:krotov_proto_update}.
It is also possible to derive update equations for choices of $g_a$ that include
spectral constraints~\cite{JosePRA13, ReichJMO2014}.

The functional $J_T$ enters the first-order update equation only in the boundary
condition for the backward propagated co-state, Eq.~\eqref{eq:chi_boundary}.
For the standard functionals defined in Eq.~\eqref{eq:JTsm} and
Eq.~\eqref{eq:JTre}, this evaluates to
\begin{align}
  - \krotovdifquo[\JTsm]{\Bra{\phi_k}}{\phi_k^{(i)}(T)}
 &= \left( \frac{1}{N^2} \sum_{l=1}^N \tau_l \right) \Op{O} \Ket{k}\,,
 \label{eq:chi_JTsm}
 \\
  - \krotovdifquo[\JTre]{\Bra{\phi_k}}{\phi_k^{(i)}(T)}
 &= \frac{1}{2N} \Op{O} \Ket{k}\,.
 \label{eq:chi_JTre}
\end{align}

If the $\Op{H}$ depends more than linearly on the field, the derivative
$$\dHdEps$$ yields an explicit dependence on $\epsilon^{(i+1)}(t)$ on the right
hand side of Eq.~\eqref{eq:krotov_first_order_update}. In this case, the usual
approach is to enforce $\epsilon^{(i+1)}(t) \approx \epsilon^{(i+1)}(t)$ with
a large value of $\lambda_a$. Alternatively, $\Delta\epsilon(t)$ may be
determined in a self-consistent loop.
This is especially relevant if instead of $\epsilon(t)$ directly,
a parametrization $\epsilon(u(t))$ is used, where $u(t)$ is the optimized
control field.
For example, $\epsilon(t) = u^2(t)$ is used to ensure that $\epsilon(t) > 0$,
and
\begin{equation}
    \epsilon(t) = \frac{\epsilon_{\max} - \epsilon_{\min}}{2} \tanh(u(t))
                + \frac{\epsilon_{\max} + \epsilon_{\min}}{2}
\end{equation}
keeps $\epsilon(t)$ bounded between $\epsilon_{\min}$ and
$\epsilon_{\max}$~\cite{MullerQIP11}.
\index{Krotov!pulse parametrization}

\begin{figure}[tb]
  \centering
  \includegraphics{krotovscheme}
\index{Krotov!update scheme}%
  \caption{Sequential update scheme in Krotov's method on a time grid.}
  \label{fig:krotovscheme}
\end{figure}
Discretization to a time grid yields the numerical scheme shown in
Fig.~\ref{fig:krotovscheme}, and resolves the seeming contradiction that the
calculation of $\epsilon^{(i+1)}(t)$ requires knowledge of the states
$\ket{\Psi_k^{(i+1)}(t)}$ propagated under $\epsilon^{(i+1)}(t)$. The scheme
starts with $\ket{\chi_k(T)}$ obtained from Eq.~\eqref{eq:chi_boundary}, which is
backward-propagated under Eq.~\eqref{eq:bw_eqm}. All backward-propagated
states $\ket{\chi(t)}$ must be stored. The first pulse value is updated
according to Eq.~\eqref{eq:krotov_first_order_update}, using $\ket{\chi_k(0)}$
and the known initial state $\ket{\Psi_k(0)} = \ket{k}$. Then, $\ket{\Psi_k(0)}$
is forward-propagated by one time step under Eq.~\eqref{eq:fw_eqm} using the
updated pulse value.  The updates proceeds sequentially, until the final
forward-propagated state $\ket{\Psi_k(T)}$ is reached.
For numerical stability, it is useful to define the normalized
\begin{equation}
  \ket{\Psi_k^{\text{bw}}(T)} = \frac{1}{|\chi_k|} \ket{\chi_{k}(T)}
\end{equation}
and then later multiply again with $|\chi_k|$ when calculating the pulse update.

At first glance, there is a striking similarity between the Krotov update
formula~\eqref{eq:krotov_first_order_update} and the gradient in
Eq.~\eqref{eq:grape_gradient}, except that for GRAPE/LBFGS-B \emph{both} backward-
and forward-propagation are performed with the same pulse $\epsilon^{(i)}(t)$.
However, a closer look shows the key difference between GRAPE/LBFGS-B and
Krotov's method: whereas the former is inherently discrete, the latter gives
a continuous update equation that is discretized only afterwards. In fact the
monotonic convergence of Krotov's method is only guaranteed in the continuous
limit; a coarse time step must be compensated by larger values of $\lambda_a$,
slowing down convergence. Generally, choosing $\lambda_a$ too small will lead to
numerical instabilities and unphysical features in the optimized pulse. A lower
limit for $\lambda_a$ can be determined from the requirement, and the change
$\Delta\epsilon(t)$ should be at most on the same order of magnitude as the
guess pulse $\epsilon^{(i)}(t)$ for that iterations. The Cauchy-Schwarz
inequality applied to the update equation~\eqref{eq:krotov_first_order_update}
yields
\begin{equation}
  \Norm{\Delta \epsilon(t)}_{\infty}
  \le
  \frac{\Norm{S(t)}}{\lambda_a}
  \sum_{k} \Norm{\chi_k}_{\infty} \Norm{\Psi_k}_{\infty}
  \Norm{\frac{\partial \Op{H}}{\partial \epsilon}}_{\infty}
  \stackrel{!}{\le}
  \Norm{\epsilon(t)^{(i)}}_{\infty}
\end{equation}
Since $S(t) \in [0,1]$ and $\ket{\Psi_k}$ is normalized, the condition for
$\lambda_a$ becomes
\begin{equation}
  \lambda_a \ge
  \frac{1}{\max\Abs{\epsilon^{(i)}(t)}}
  \left[ \sum_{k} \Norm{\chi_k}_{\infty} \right]
  \Norm{\frac{\partial \Op{H}}{\partial \epsilon}}_{\infty}\,.
\end{equation}
From a practical point of view, the best strategy is to start the optimization
with a conservatively large value of $\lambda_a$, and after a few iterations
lower $\lambda_a$ as far as possible without introducing numerical
instabilities. The value of $\lambda_a$ may be adjusted dynamically with the
rate of convergence. Generally, the optimal choice of $\lambda_a$ requires some
trial and error. Inspired by gradient ascent, it has been proposed to employ
quasi-Newton methods to determine the Krotov step width $\lambda_a$
dynamically~\cite{EitanPRA11}.

When using the rotating wave approximation (RWA),
cf.~appendix~\ref{AppendixRWA}, it is important to remember that the target
transformation $\Op{O}$ is usually defined in the lab frame, not in the rotating
frame. This is relevant for the construction of $\ket{\chi_k(T)}$. The easiest
approach is to transform the result of the forward propagation $\ket{\Psi_k(T)}$
from the rotating frame to the lab frame, then construct $\ket{\chi_k(T)}$ for
the next OCT iteration, and transform $\ket{\chi_k(T)}$ back to the rotating
frame, before starting the backward-propagation for the next OCT iteration.
When the RWA is used, the control-pulses are complex-valued. In this case, the
Krotov update equation is valid for both the real and the imaginary part
independently. That is, in the update for the real part of the pulse, all
derivatives are also taken with respect to only the real part, and likewise
for the imaginary part.

The control equations have been written mostly in the notation of Hilbert space.
However, They are, equally valid for a gate optimization in Liouville space, by
replacing states with density matrices, $\Op{H}$ with $\Liouville$, and inner
products with Hilbert-Schmidt products. An explicit formulation is given in
chapters~\ref{chap:robust} and~\ref{chap:3states}.

\subsection{Choosing an Optimization Method}

Whether to use a gradient-free optimization method, gradient ascent, or Krotov's
method depends on the size of the problem (both the Hilbert space and the number
of control parameters), the requirements on the control pulse, and
the optimization functional. Gradient-free methods should be used if propagation
is extremely cheap (small Hilbert space dimension), the number of independent
control parameters is relatively small, or the functional is in a form that does
not allow to calculate gradients.

Gradient ascent should be used if the control parameters are discrete, such as
on a coarse-grained time grid, and the derivative of $J$ with respect to each
control parameter is known. Moreover, evaluation of the gradient must be
numerically feasible; for example, the parametrization on a time grid shown in
section~\ref{subsec:gradient_ascent} yields an efficient scheme, but
parametrization on a frequency grid does not.

Krotov's method should be used if the control is near-continuous, and if the
derivative of $J_T$ with respect to the states, Eq.~\eqref{eq:chi_boundary}, can
be calculated. Sometimes, a functional must be rewritten in an alternative form
before this is possible, as illustrated in chapter~\ref{chap:pe}. When these
conditions are met, Krotov's method gives excellent convergence, although it
is often observed to slow down when getting close to the minimum of
$J$. Since gradient ascent does not show such a slow-down, it can be beneficial
to switch from Krotov's method to gradient ascent in the final stage of the
optimization.

