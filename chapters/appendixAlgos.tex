\chapter{Propagation Algorithms}
\label{AppendixAlgos}

\section{Chebychev Propagator}

For the evaluation of the time evolution operator
$f(\pm\hat{A}\,dt) = \ee^{\pm\ii \hat{A}\,dt}$, the Chebychev propagation is
implemented by Algorithm~\ref{al:ChebyProp}. The operator $\hat{A}$ may be
a Hamiltonian, in which case the state vectors $\vec{v}$ are Hilbert space
states, or $\hat{A}$ may be a Liouvillian with no dissipators, in which case
$\vec{v}$ is a density matrix.

A re-calculation of the Chebychev coefficients in
line~\ref{Cheby:CallExpChebyCeoffs} is necessary only if the spectral
radius $\Delta$ and minimum eigenvalue $E_{\min}$ have changed, otherwise, the
coefficients from a previous calculation can be re-used. For a more general
function $f(\hat{A})$, instead of \Call{ExpChebyCoeffs}{}, the routine
\Call{ChebyCoeffs}{} defined in Algorithm~\ref{al:ChebyCoeffs} may be used,
together with an appropriate pre-factor in lines~\ref{Cheby:H_norm1}
and~\ref{Cheby:H_norm2}. The coefficients $[a_0 \dots a_n]$ are the same for
both a forward and a backward propagation.
They are real for the exponential function, but may be complex for an arbitrary
$f(\hat{A})$.

\begin{algorithm}
  \caption{{\sc Chebychev-Propagator}
  Evaluate $\vec{w} = f(\pm\hat{A}\,dt) \vec{v}$,
  with $f(\pm\hat{A}\,dt) = \ee^{\pm\ii \hat{A}\,dt}$.
  \label{al:ChebyProp}
  }
  \begin{algorithmic}[1]
    \Statex
    \Require{input vector $\vec{v} \in \Complex^N$;
             operator $\hat{A} \in \Complex^{N \times N}$;
             time step $dt$;}
    \Ensure{Approximation of propagated vector
            $\vec{w}= e^{-i \hat{A}\,dt} \vec{v} \in \Complex^N$}
    \Statex
    \Procedure{Cheby}{$\vec{v}$, $\hat{A}$, $dt$}
     \State $\Delta =$ spectral radius of $\hat{A}$
     \State $E_{\min} =$ minimum eigenvalue of $\hat{A}$
     \State $[a_0 \dots a_n] =$ \Call{ExpChebyCoeffs}{$\Delta$, $E_{\min}$, $dt$}
            \label{Cheby:CallExpChebyCeoffs}
     \State $d = \frac{1}{2} \Delta$;
            $\beta = d + E_{\min}$
     \State $\vec{v}_0 = \vec{v}$
     \State $\vec{w}^{(0)} = a_0 \vec{v}_0$
     \State $\vec{v}_1 = \pm\frac{\ii}{d}\left(
                            \hat{A} \vec{v}_0 - \beta \vec{v}_0
                         \right)$
            \label{Cheby:H_norm1}
     \State $\vec{w}^{(1)} = \vec{w}^{(0)} + a_1 \vec{v}_1$
     \For{$i = 2:n$}
       \State $\vec{v}_i = \pm \frac{2 \ii}{d}\left(
                            \hat{A} \vec{v}_{i-1} - \beta \vec{v}_{i-1}
                            \right) + \vec{v}_{i-2}$
              \label{Cheby:H_norm2}
       \State $\vec{w}^{(i)} = \vec{w}^{(i-1)} + a_i \vec{v}_i$
     \EndFor
     \State \Return $e^{\pm \ii\,\beta\,dt} \vec{w}^{(n)}$
   \EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{{\sc ChebychevCoefficients}
  for $f(\pm \hat{A}\,dt) = e^{\pm i \hat{A}\,dt}$.
  \label{al:ExpChebyCoeffs}
  }
  \begin{algorithmic}[1]
    \Statex
    \Require{spectral radius $\Delta$ of $\hat{A}$;
             minimum eigenvalue $E_{\min}$ of $\hat{A}$;
             time step $dt$}
    \Ensure{Array of Chebychev coefficients $[a_0\dots a_n]$ allowing to
    approximate $f(\hat{A}\,dt)$ to pre-defined precision.}
    \Statex
    \Procedure{ExpChebyCoeffs}{$\Delta$, $E_{\min}$, $dt$}
     \State $\alpha = \frac{1}{2} \Delta \, dt$
     \State $a_0 = J_0(\alpha)$
     \Comment 0'th order Bessel-function of first kind
     \For{$i=1: n_{\max} \approx 4 \lfloor\alpha\rfloor$}
       \State $a_{i} = 2 J_i(\alpha)$
       \Comment $i$'th order Bessel-function of first kind
       \IIf{$\Abs{a_i} < $~limit} exit loop with $n=i$
     \EndFor
     \State \Return $[a_0, \dots a_n]$
   \EndProcedure
  \end{algorithmic}
\end{algorithm}

\clearpage


\section{Newton Propagator with Restarted Arnoldi}

For the evaluation of the time evolution operator
$f(\pm\hat{A}\,dt) = \ee^{\pm\ii \hat{A}\,dt}$, where $\hat{A}$ has complex
eigenvalues, the Newton algorithm using a restarted Arnoldi scheme, as discussed
in section~\ref{subsec:newton} in chapter~ref{chap:numerics} can be used. The
operator $\hat{A}$ is typically a dissipative Liouvillian, but may also be
a Hamiltonian with non-Hermitian terms. The propagation scheme is implemented by
algorithm~\ref{al:RestartedNewton}, using the Arnoldi
algorithm~\ref{al:Arnoldi} as a central component.

The algorithm proceeds in iterations, with each iteration adding $m$ new terms
to the Newton expansion of the propagator. In each iteration, starting from
a vector $\vec{v}_s$, the Arnoldi algorithm is performed to obtain the Arnoldi
vectors spanning the $m$-dimensional Krylov subspace, as well as the Hessenberg
matrix $\hat{H}$, i.e.\ the projection of $\hat{A}$ into the Krylov space. From
the eigenvalues of $\hat{H}$ (the ``Ritz values''),
\index{Ritz values}
new sampling points for the Newton polynomials are chosen, in Leja ordering to
maximize numerical stability, using algorithm~\ref{al:ExtendLeja}. Then, new
Newton coefficients are calculated using
algorithm~\ref{al:ExtendNewtonCoeffs}. Lastly, the coefficients and Leja points
are used to calculate the contribution to the propagation result, and the
starting vector for the next iteration.

\begin{algorithm}
  \caption{{\sc RestartedNewton:}
  Evaluate $\vec{w} = f(\pm\hat{A}\,dt) \vec{v}$,
  with $f(\pm\hat{A}\,dt) = \ee^{\pm \ii \hat{A}\,dt}$.
  \label{al:RestartedNewton}
  }
  \begin{algorithmic}[1]
    \Statex
    \Require{input vector $\vec{v} \in \Complex^N$;
             operator $\hat{A} \in \Complex^{N \times N}$;
             time step $dt$; maximum size $m$ of Hessenberg matrices}
    \Ensure{Approximation of propagated vector
            $\vec{w}= \ee^{\pm \ii \hat{A}\,dt} \vec{v} \in \Complex^N$}
    \Statex
    \Procedure{RestartedNewton}{$\vec{v}$, $\hat{A}$, $dt$, $m$}
    \State $A_0 = \emptyset$; $Z_0 = \emptyset$
    \Comment{$A$, $Z$ use zero-based indexing}
    \State $\vec{w}^{(0)} = \vec{0} \in \Complex^N$
    \State $\vec{v}_0 = \vec{v} \in \Complex^N$
    \State $\beta = \Norm{\vec{v}_0}$
    \State $\vec{v_0} = \vec{v}_0/\beta$
    \State $s = 0$
    \While{not converged}
    \Comment{Iteration $s \rightarrow s+1$}
      \State $U$, $\hat{H}$, $Z$, $m$ = \Call{Arnoldi}{$\hat{A}$, $dt$, $\vec{v}_{s}$, $m$} \label{RN:arnoldi_call}
      \If{$m = 0$, $s = 0$} \label{RN:eigenstate}
        \State \Return $\ee^{\pm \ii \beta H_{1,1}} \vec{v}_s$
      \EndIf
      \State Normalize $Z$ with center $c$ and radius $\rho$
             \label{RN:normalization}
      \State $n_s = \Abs{Z_{s}}$
      \State $Z_{s+1} =$ \Call{ExtendLeja}{$Z_{s}, Z, m$}
             \label{RN:ExtendLejaCall}
      \State $A_{s+1} =$ \Call{ExtendNewtonCoeffs}{$A_{s}, Z_{s+1}, \rho, c$}
             \label{RN:ExtendNewtonCoeffsCall}
      \State $\vec{r}_{0} = \beta \vec{e}_1 \in \Complex^{m+1}$
      \Comment $\vec{e}_1$ is unit vector
      \State $\vec{p}_{0} = a_{n_s} \vec{r}_{0}$
      \For{k=1:m-1}
        \State
          $\vec{r}_{k}
           = (\frac{1}{\rho}\hat{H} - (z_{n_s+k-1}) +\frac{c}{\rho})
           \vec{r}_{k-1}$
        \State $\vec{p}_{k}
                = \vec{p}_{k-1} + a_{n_s+k} \vec{r}_{k}$
      \EndFor \label{RN:end_r_loop}
      \State $w_{+}^{(s)} = \sum_{i=1}^{m} \left[\vec{p}_{m-1}\right]_{i} \, \vec{u}_i$
      \Comment{$\vec{u}_i \in U$}
      \State $\vec{w}^{(s+1)} = \vec{w}^{(s)} + \vec{w}_{+}^{(s)}$
      \State  $\vec{r}_{m}
              =  \left(
                  \frac{1}{\rho}\hat{H}
                  - \left(z_{n_s+m-1} + \frac{c}{\rho}\right) \identity
                \right)\vec{r}_{m-1}$
      \State $\beta = \Norm{\vec{r}_{m}}$
      \State $\vec{r}_{m} = \vec{r}_{m} / \beta$
      \State $\vec{v}_{s+1} = \sum_{i=1}^{m+1}
                              \left[ \vec{r}_m\right]_{i} \, \vec{u}_i$
      \Comment{$\vec{u}_i \in U$}
      \State $s = s + 1$
      \State converged if
             $\frac{\Norm{\vec{w}_{+}^{s}}}{\Norm{\vec{w}^{s}}} < $~limit
    \EndWhile
    \State \Return $\vec{w}^{(s)}$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

Moreover:
\begin{itemize}[noitemsep]

  \item The check in line~\ref{RN:eigenstate} catches if $\vec{v}_s$ is an
  eigenstate of $\hat{A}$.

  \item For the normalization in line~\ref{RN:normalization}, the
  normalization radius $\rho$ and center $c$  should be calculated in the
  \emph{first} iteration and the re-used in subsequent iterations. For the
  set of Leja points $Z_0 = [z_{0}:z_{m-1}]$, they are calculated as
  \begin{equation}
    c = \frac{1}{m} \sum_{j=0}^{m-1} z_j\,, \qquad
    \rho = \Pi_{j=0}^{m-1} \Abs{c - z_j}^{\frac{1}{m}}\,.
  \end{equation}
  %Alternatively, to ensure that all the Leja points are in the unit circle, use
  %\begin{equation}
  %  \rho = \max_j \Abs{c - z_j}
  %\end{equation}

  \item The call to \Call{ExtendLeja}{} in line~\ref{RN:ExtendLejaCall}
  adds Leja points $z_{n_s}\dots z_{n_s+m-1}$.

  \item The call to \Call{ExtendNewtonCoeffs}{} in
  line~\ref{RN:ExtendNewtonCoeffsCall} adds the coefficients $a_{n_s}\dots
  a_{n_s+m-1}$

  \item The loop ending in line~\ref{RN:end_r_loop} implements the formula
  \begin{gather}
    \vec{r}_{m-1} = \Pi_{j=1}^{k}(\hat{H} - z_{n_s+j-1} \identity)\vec{e}_1\,, \\
    \vec{p}_{m-1} = \sum_{k=0}^{m-1} a_{n_s+k} \, \vec{r}_{k}\,.
  \end{gather}

\end{itemize}

\clearpage

\begin{algorithm}
  \caption{{\sc Arnoldi:}
  Obtain the $m \times m$ Hessenberg matrix for an operator $\hat{A} dt$ by
  projecting it into the Krylov space starting from a vector $\vec{v}$.
  \label{al:Arnoldi}
  }
  \begin{algorithmic}[1]

    \Statex
    \Require{%
    Operator $\hat{A}$,
    Time step $dt$;
    input vector $\vec{v}$;
    maximum order $m_{\max}$}

    \Ensure{%
    Array of $m+1$ (extended) Arnoldi vectors, each of the same dimension as
    $\vec{v}$; Extended $(m+1) \times (m+1)$ Hessenberg matrix $\hat{H}$;
    Accumulated Ritz values $Z$};
    Dimension $m$ of Hessenberg matrix
    \Statex

    \Procedure{Arnoldi}{$\hat{A}$, $dt$, $\vec{v}$, $m_{\max}$}
      \State $\beta = \Norm{\vec{v}}$; $\vec{u}_1 = \vec{v} / \beta$;
              $U = [ \vec{u}_{1} ]$; $Z = \emptyset $; $m = m_{\max}$
      \State $\hat{H}_{1:(m+1),1:(m+1)} = 0$
      \For{$j = 1:m_{\max}$}
        \State $\vec{u}_{j+1} = \hat{A} \vec{u}_j$
        \For{$i = 1:j$}
          \State $H_{i,j} = dt \, \Braket{\vec{u}_i | \vec{u}_{j+1}}$
                 \label{Ar:Hess_element}
          \State $\vec{u}_{j+1} = \vec{u}_{j+1} - \frac{H_{i,j}}{dt} \vec{u}_i$
        \EndFor \label{Ar:EndGramSchmidt}
        \State $Z = Z \cup \text{eigenvalues}(\hat{H}_{1:j,1:j})$
               \label{Ar:eigenvalues}
        \State $h_{next} = \Abs{\vec{u}_{j+1}}$
        \State if $h_{next} \approx 0$: $m = j$, exit loop
        \State $\vec{u}_{j+1} = \vec{u}_{j+1} / h_{next}$
        \State $U = U \cup \vec{u}_{j+1}$
        \State $H_{j+1,j} = h_{next} \, dt$
      \EndFor
      \State \Return $U = [ \vec{u}_{1} : \vec{u}_{m+1}]$,
                     $\hat{H}_{1:(m+1), 1:(m+1)}$, $Z$, $m$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

Notes:
\begin{itemize}[noitemsep]

  \item At the end of the loop in line~\ref{Ar:EndGramSchmidt}, $\hat{H}$ is
  complete as a $j \times j$ matrix. The loop itself is a Gram-Schmidt
  orthonormalization of the Arnoldi vectors $\vec{u}_j$.

  \item The eigenvalues of the Hessenberg matrix in line~\ref{Ar:eigenvalues}
  can be obtained with the QR method, implemented in LAPACK as {\tt ZHSEQR}.
  %\Comment{sub-matrix $\hat{H}_{1:m,1:m}$ is proper Hessenberg matrix}

\end{itemize}


\begin{algorithm}
  \caption{{\sc ExtendLeja:}
  Choose $m$ new points from a set of Ritz values to extend an existing array of
  Leja points
  \label{al:ExtendLeja}
  }
  \begin{algorithmic}[1]

    \Statex

    \Require{%
    Array $Z_s$ of $n_s$ existing Leja points;
    Array $Z$ of new candidate points (Ritz values);
    Number $m$ of points to pick from $Z$.}

    \Ensure{%
    Array $Z_{s+1}$ of $n_s + m$ Leja-ordered points}
    \Statex

    \Procedure{ExtendLeja}{$Z_s, Z, m$}
      \State $n_0 = 1$; $Z_{s+1} = Z_{s}$
      \If{$Z_{s} = \emptyset$}
        \State $z = \max(\abs(Z))$
        \State $Z_{s+1} = Z_{s_{1}} \cup z$; remove $z$ from $Z$
        \State $n_0 = 2$
      \EndIf
      \For{$n = n_0:m$}
        \State Select $z_i \in Z$ that maximizes
               $\Pi_{z_j \in Z_{s+1}} \Abs{z_i - z_j}$
        \State $Z_{s+1} = Z_{s+1} \cup z_i$; remove $z_i$ from $Z$
      \EndFor
      \State \Return $Z_{s+1}$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{{\sc ExtendNewtonCoeffs:}
  Choose $m$ new points from a set of Ritz values to extend an existing array of
  Leja points
  \label{al:ExtendNewtonCoeffs}
  }
  \begin{algorithmic}[1]

    \Statex

    \Require{%
    Array $A_{s} = [a_{0} \dots a_{n_s-1} ]$ of $n_s$ Newton coefficients from
    previous iteration;
    Array $Z_{s+1} = [z_{0} \dots z_{n_s-1+m}]$ of Leja points;
    Normalization radius $\rho$; Normalization center $c$
    }

    \Ensure{%
    Array $A_{s+1} = [a_{0} \dots \dots a_{n_s-1+m}]$ of $n_s+m$ Newton
    coefficients
    }
    \Statex
    \Procedure{ExtendNewtonCoeffs}{$A_{s}, Z_{s+1}, \rho, c$}
      \State $A_{s+1} = A_{s}$;
             $n_0 = n_s = \Abs{A_{s}}$,
             $m = \Abs{Z_{s+1}} - n_s$;
      \State Define $f(z) = \ee^{\pm\ii (\rho z + c)}$
      \If{$n_s = 0$}
        \State $a_{0} = f(z_0)$
        \State $A_{s+1} = A_{s+1} \cup a_{0}$
        \State $n_0 = 1$
      \EndIf
      \For{$k = n_0:n_s-1+m$}
        \State $a_k = \frac{f(z_k) - a_0 - \sum_{n=1}^{k-1} a_{n} \Pi_{j=0}^{n-1} (z_k - z_j)}
                           {\Pi_{j=0}^{k-1} (z_k - z_j)}$
        \State $A_{s+1} = A_{s+1} \cup a_k$
      \EndFor
      \State \Return $A_{s+1}$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

